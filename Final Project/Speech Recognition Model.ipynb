{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "import torchaudio\n",
    "from torch.utils import data\n",
    "import editdistance\n",
    "import soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextTransform:\n",
    "#     \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "#     def __init__(self, char_map_str):\n",
    "#         self.char_map_str = char_map_str\n",
    "#         self.char_map = {}\n",
    "#         self.index_map = {}\n",
    "#         for line in self.char_map_str.strip().split('\\n'):\n",
    "#             ch, index = line.split()\n",
    "#             self.char_map[ch] = int(index)\n",
    "#             self.index_map[int(index)] = ch\n",
    "#         self.index_map[1] = ' '\n",
    "\n",
    "#     def text_to_int(self, text):\n",
    "#         \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "#         int_sequence = []\n",
    "#         for c in text:\n",
    "#             if c == ' ':\n",
    "#                 ch = self.char_map['']\n",
    "#             else:\n",
    "#                 ch = self.char_map[c]\n",
    "#             int_sequence.append(ch)\n",
    "#         return int_sequence\n",
    "\n",
    "#     def int_to_text(self, labels):\n",
    "#         \"\"\" Use a character map and convert integer labels to a text sequence \"\"\"\n",
    "#         string = []\n",
    "#         for i in labels:\n",
    "#             string.append(self.index_map[i])\n",
    "#         return ''.join(string).replace('', ' ')\n",
    "\n",
    "# # Define the char_map_str\n",
    "# char_map_str = \"\"\"\n",
    "#  ' 0\n",
    "#  <SPACE> 1\n",
    "#  a 2\n",
    "#  b 3\n",
    "#  c 4\n",
    "#  d 5\n",
    "#  e 6\n",
    "#  f 7\n",
    "#  g 8\n",
    "#  h 9\n",
    "#  i 10\n",
    "#  j 11\n",
    "#  k 12\n",
    "#  l 13\n",
    "#  m 14\n",
    "#  n 15\n",
    "#  o 16\n",
    "#  p 17\n",
    "#  q 18\n",
    "#  r 19\n",
    "#  s 20\n",
    "#  t 21\n",
    "#  u 22\n",
    "#  v 23\n",
    "#  w 24\n",
    "#  x 25\n",
    "#  y 26\n",
    "#  z 27\n",
    "# \"\"\"\n",
    "\n",
    "# # Initialize TextTransform with char_map_str\n",
    "# text_transform = TextTransform(char_map_str)\n",
    "\n",
    "# # Define train_audio_transforms, valid_audio_transforms, and data_processing as in the original code\n",
    "# train_audio_transforms = nn.Sequential(\n",
    "#     torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "#     torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "#     torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    "# )\n",
    "\n",
    "# valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "# def data_processing(data, data_type=\"train\"):\n",
    "#     spectrograms = []\n",
    "#     labels = []\n",
    "#     input_lengths = []\n",
    "#     label_lengths = []\n",
    "#     for (waveform, _, utterance, _, _, _) in data:\n",
    "#         if data_type == 'train':\n",
    "#             spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "#         else:\n",
    "#             spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "#         spectrograms.append(spec)\n",
    "#         label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "#         labels.append(label)\n",
    "#         input_lengths.append(spec.shape[0] // 2)\n",
    "#         label_lengths.append(len(label))\n",
    "\n",
    "#     spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "#     labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "#     return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: ' 0\n",
      "Line: <SPACE> 1\n",
      "Line: a 2\n",
      "Line: b 3\n",
      "Line: c 4\n",
      "Line: d 5\n",
      "Line: e 6\n",
      "Line: f 7\n",
      "Line: g 8\n",
      "Line: h 9\n",
      "Line: i 10\n",
      "Line: j 11\n",
      "Line: k 12\n",
      "Line: l 13\n",
      "Line: m 14\n",
      "Line: n 15\n",
      "Line: o 16\n",
      "Line: p 17\n",
      "Line: q 18\n",
      "Line: r 19\n",
      "Line: s 20\n",
      "Line: t 21\n",
      "Line: u 22\n",
      "Line: v 23\n",
      "Line: w 24\n",
      "Line: x 25\n",
      "Line: y 26\n",
      "Line: z 27\n"
     ]
    }
   ],
   "source": [
    "char_map_str = \"\"\"\n",
    "' 0\n",
    "<SPACE> 1\n",
    "a 2\n",
    "b 3\n",
    "c 4\n",
    "d 5\n",
    "e 6\n",
    "f 7\n",
    "g 8\n",
    "h 9\n",
    "i 10\n",
    "j 11\n",
    "k 12\n",
    "l 13\n",
    "m 14\n",
    "n 15\n",
    "o 16\n",
    "p 17\n",
    "q 18\n",
    "r 19\n",
    "s 20\n",
    "t 21\n",
    "u 22\n",
    "v 23\n",
    "w 24\n",
    "x 25\n",
    "y 26\n",
    "z 27\n",
    "\"\"\"\n",
    " \n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self, char_map_str):\n",
    "        self.char_map_str = char_map_str\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in self.char_map_str.strip().split('\\n'):\n",
    "            print(\"Line:\", line)  # Add this line for debugging\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']  # Use '<SPACE>' key for space character\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('', ' ')\n",
    "\n",
    "text_transform = TextTransform(char_map_str)\n",
    "\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "# text_transform = TextTransform()\n",
    "\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "        \n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=28).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : neat_grade_4875\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/brody-adams21/speechrecognition/442b49fb6e614c7e8da299371cca9ebf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cer                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_loss [3]            : (3.0814789930979414, 3.809693892796835)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_learning_rate [18] : (2.0000000000314905e-09, 0.00047619057142857143)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [18]          : (2.905561685562134, 7.779111385345459)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     wer                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 40\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout       : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs        : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.0005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_class       : 29\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_cnn_layers  : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_feats       : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_rnn_layers  : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rnn_dim       : 512\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stride        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'c:\\\\Users\\\\brody\\\\Machine Learning\\\\Final Project' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/brody-adams21/speechrecognition/93a03b0e86a749ab9337ad8a9d26eaff\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize experiment object\n",
    "experiment = Experiment(api_key=\"IQcdLgDZdrOWwdXP8KdNHBwjz\", project_name=\"speechrecognition\")\n",
    "experiment.set_name(\"mlFinal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(ref, hyp):\n",
    "    \"\"\" Compute the Levenshtein distance between two lists \"\"\"\n",
    "    m, n = len(ref), len(hyp)\n",
    "    # Create a distance matrix\n",
    "    D = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Initialize the edge values\n",
    "    for i in range(m + 1):\n",
    "        D[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        D[0][j] = j\n",
    "\n",
    "    # Populate matrix\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            D[i][j] = min(\n",
    "                D[i - 1][j] + 1,   # Deletion\n",
    "                D[i][j - 1] + 1,   # Insertion\n",
    "                D[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    return D[m][n]\n",
    "\n",
    "def cer(target, prediction):\n",
    "    target = target.replace(\" \", \"\")\n",
    "    prediction = prediction.replace(\" \", \"\")\n",
    "    distance = levenshtein_distance(target, prediction)\n",
    "    return distance / max(len(target), len(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "\n",
    "def wer(pred, target):\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER).\n",
    "    \n",
    "    Returns 0.0 if either input is an empty string, indicating potential silence or\n",
    "    non-speech segments where WER calculation does not make sense.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are strings and remove leading/trailing whitespace\n",
    "    pred = str(pred).strip()\n",
    "    target = str(target).strip()\n",
    "\n",
    "    # Check for empty strings\n",
    "    if not pred or not target:\n",
    "        # Option 1: Return worst case scenario (100% error)\n",
    "        return 1.0\n",
    "    \n",
    "        # Option 2: Return special code or throw an exception to handle it differently\n",
    "        # raise ValueError(\"Empty strings provided for WER calculation.\")\n",
    "        # return \"Invalid\"  # or any indicator to handle these cases distinctly\n",
    "\n",
    "    # Assuming inputs are now valid and non-empty\n",
    "    return jiwer.wer(target, pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : mlFinal\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/brody-adams21/speechrecognition/93a03b0e86a749ab9337ad8a9d26eaff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : mlFinal\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    }
   ],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    with experiment.train():\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
    "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            iter_meter.step()\n",
    "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                print(r'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(spectrograms), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
    "    print('\\nevaluating…')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with experiment.test():\n",
    "        with torch.no_grad():\n",
    "            for I, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
    "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
    "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "\n",
    "\n",
    "def main(\n",
    "        learning_rate=5e-4, batch_size=20, epochs=10,\n",
    "        train_url=\"train-clean-100\", test_url=\"test-clean\",\n",
    "        experiment=Experiment(api_key='', disabled=True)):\n",
    "    \n",
    "\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    experiment.log_parameters(hparams)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "    \n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
    "        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'c:\\\\Users\\\\brody\\\\Machine Learning\\\\Final Project' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/brody-adams21/speechrecognition/c3741943eb9e4e0b8e7a55b471c5f950\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 23705373\n",
      "Train Epoch: 1 [0/1781 (0%)]\\tLoss: 7.339015\n",
      "\n",
      "evaluating…\n",
      "Test set: Average loss: 2.9946, Average CER: 1.000000 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 2 [0/1781 (0%)]\\tLoss: 2.888527\n",
      "\n",
      "evaluating…\n",
      "Test set: Average loss: 2.9606, Average CER: 1.000000 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 3 [0/1781 (0%)]\\tLoss: 2.872975\n",
      "\n",
      "evaluating…\n",
      "Test set: Average loss: 2.9604, Average CER: 1.000000 Average WER: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(learning_rate=5e-4, batch_size=40, epochs=3, train_url=\"train-clean-100\", test_url=\"test-clean\", experiment = Experiment(api_key=\"IQcdLgDZdrOWwdXP8KdNHBwjz\", project_name=\"speechrecognition\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
